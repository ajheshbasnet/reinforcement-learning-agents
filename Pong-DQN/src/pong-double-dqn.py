# -*- coding: utf-8 -*-
"""ATARI-DQN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/ajheshbasnet/atari-dqn.40f211ab-f4bb-46da-a4f8-6fdce6a6a3b8.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20260103/auto/storage/goog4_request%26X-Goog-Date%3D20260103T142136Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1eec6ee26b79d1402a893ecd8edeae9b3e2b2244dad8b9d589be599632ca0c9ba71d96d97e160142b569feeb3b3b231acfecd6d64339430f1f02cc0b59ec3b091c52c5cebef3493a5f0c94d1442a61d376318d4638b6134a4888187ccee8cf8883dbc9debfdd8657349f13704a06aec820d95e2b6017b50a0e3395fc29212d171e34df069ac0cef7711c767a5e99034b72f6a76c36d5c0559753d7a0f5642a6ed10d29dcd17e2b61b3313cf24a2569bc703478d9cc21e27e335de743ae2d56a56c5fcb3c5d1ff3c27a141a83184ebf9db2c0dba55ba286302678ad1c6364c659f431e0e0713a996cd02494a7e2a6aac15981d19c3f9fcc5d2c3d19fb39da0ae5
"""

!pip install gymnasium[atari]
#!python ale_py.roms install
!pip install ale_py
#!pip install gymnasium[atari,accept-rom-license]

# !pip install minatar

import ale_py
import gymnasium as gym
#import minatar
# print(gym.registry) -> to see all the games inside gymnasium openai library
print("BreakoutNoFrameskip-v4" in gym.registry)

import torch
import torch.nn as nn
import random
import numpy as np
import torch.nn.functional as F
import math
from collections import deque
from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation, RecordVideo
from dataclasses import dataclass
import matplotlib.pyplot as plt
from tqdm import tqdm
import wandb

def create_run(configs):

  wandb.login(key="ee0683c540665b9502ef44b224c3796a0e8a907f")

  run = wandb.init(
    name = "dqn-run",
    project="pingpong",
    config=vars(configs),
  )
  return run

def createEnvironment(name: str):

  env = gym.make(name, frameskip = 1, full_action_space=False, render_mode="rgb_array", max_episode_steps=configs.N_STEP)

  env = AtariPreprocessing(env, frame_skip=4, grayscale_obs=True, screen_size = 84)
  # "scale_obs" means the pixels are scaled/normalised from 0 to 1 else it's in uint8 number--> keeping it False because to store it the float32 takes way huge memory so the training will be too much slow around 11s/iteration. Hence do it during the run time only.

  env = FrameStackObservation(env, 4)
  # it gives [frame(t-3), frame(t-2), frame(t-1), frame(t)] NOT [frame(t), frame(t+1), frame(t+2), frame(t+3)]

  # during env.reset() it gives obs = stack of [obs, obs, obs, obs] which is the same frame during the first time
  # so after the 1st action the stack becomes [f0, f0, f0, f1] and after another action it becomes [f0, f0, f1, f2] and so on.

  return env

print(100_000)

@dataclass

class configs():
  MAX_EPISODES: int = 2_000_000
  N_STEP: int = 10_000
  LR: float = 2.5e-4
  BATCH_SIZE: int = 64
  STACK_SIZE: int = 4
  EVAL_LOOPS: int = 3
  GAME_ID: str =  "ALE/Pong-v5"
  MEMORY_MAX_LENGTH: int = 1_000_000
  TRAINING_START_STEPS: int = 65_000
  DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
  print(f"--<< {DEVICE} is being used >>--")

configs = configs()

print(f"Total Steps = {(configs.MAX_EPISODES)/1e6} million")

env = createEnvironment(configs.GAME_ID)

state = env.reset()[0]

state.shape

torch.randint(0, env.action_space.n, (1,)).item()

print(env.unwrapped.get_action_meanings())

print(env.action_space.n)

def getAction(*, state, qnetwork, step):
    EPS_START = 1.0
    EPS_END   = 0.01
    TOTAL_STEP = configs.MAX_EPISODES

    progress = (step) / (TOTAL_STEP)
    epsilon = EPS_END + 0.5 * (1 + math.cos(math.pi * progress))
    #return epsilon

    if random.random() < epsilon:
        return env.action_space.sample(), epsilon

    with torch.no_grad():
        return qnetwork(state).argmax(dim=-1).item(), epsilon

# kk= []

# for k in range(configs.MAX_EPISODES):
#     kk.append(getAction(state = None, qnetwork = None, step = k))

# plt.plot()

class QNetwork(nn.Module):

  def __init__(self, input_dim: int, action_dim: int):
    super().__init__()
    self.conv1 = nn.Conv2d(configs.STACK_SIZE, 32, kernel_size=5, stride=4)
    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=3)
    self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)

    self.relu = nn.ReLU()
    self.linear1 = nn.Linear(1024, 512)
    self.linear2 = nn.Linear(512, action_dim)

  def forward(self, x):

    features_map = self.relu(self.conv1(x/255.0))
    features_map = self.relu(self.conv2(features_map))
    features_map = self.relu(self.conv3(features_map))

    flatten = features_map.view(x.size(0), -1)

    x = self.relu(self.linear1(flatten))
    return self.linear2(x)

  def getQvals(self, states, action):
    q_vals = self(states)
    onlineQVals = torch.gather(q_vals, -1, action)
    return onlineQVals

def init_weights(m):
    if isinstance(m, torch.nn.Conv2d):
        torch.nn.init.kaiming_normal_(m.weight, nonlinearity="relu")
        if m.bias is not None:
            torch.nn.init.constant_(m.bias, 0.0)

    elif isinstance(m, torch.nn.Linear):
        torch.nn.init.kaiming_normal_(m.weight, nonlinearity="relu")
        torch.nn.init.constant_(m.bias, 0.0)

onlineNetwork = QNetwork(4, env.action_space.n).to(configs.DEVICE)
targetNetwork = QNetwork(4,env.action_space.n).to(configs.DEVICE)

onlineNetwork.apply(init_weights)
targetNetwork.load_state_dict(onlineNetwork.state_dict())

onlineNetwork(torch.rand(1, 4, 84, 84).to(configs.DEVICE))
targetNetwork(torch.rand(1, 4, 84, 84).to(configs.DEVICE))

sum(p.numel() for p in onlineNetwork.parameters())

def get_batches(memory):

  batches = random.sample(memory, configs.BATCH_SIZE)
  obs, action, reward, next_obs, dones = zip(*batches)

  obs = torch.from_numpy(np.array(obs, dtype = np.float32)).to(configs.DEVICE).detach()
  next_obs = torch.from_numpy(np.array(next_obs, dtype = np.float32)).to(configs.DEVICE).detach()

  action = torch.tensor(action, dtype = torch.long).to(configs.DEVICE).unsqueeze(-1).detach()
  reward = torch.tensor(reward, dtype = torch.float32).to(configs.DEVICE).unsqueeze(-1).detach()
  dones = torch.tensor(dones, dtype = torch.bool).to(configs.DEVICE).unsqueeze(-1).detach()

  return obs, action, reward, next_obs, dones

def lr_lambda(steps):
  MIN_LEARNING_RATE = 5e-5
  MAX_LEARNING_RATE = configs.LR
  WARMUP_STEP = 1200
  TOTAL_STEP = configs.MAX_EPISODES*configs.N_STEP

  if steps < WARMUP_STEP:
      return steps/WARMUP_STEP

  progress = (steps - WARMUP_STEP) / (TOTAL_STEP - WARMUP_STEP)
  return MIN_LEARNING_RATE + 0.5 * (1 + math.cos(math.pi * progress))

print(configs)

Memory = deque(maxlen = configs.MEMORY_MAX_LENGTH)

OPTIMIZER = torch.optim.AdamW(onlineNetwork.parameters(), lr = configs.LR)
#SCHEDULER = torch.optim.lr_scheduler.LambdaLR(OPTIMIZER, lr_lambda)

run = create_run(configs)

def evaluationLoop(qnetwork, recordVideo = False):

  qnetwork.eval()

  eval_env = createEnvironment(name = configs.GAME_ID)

  if recordVideo:
    eval_env = RecordVideo(
                           eval_env, video_folder="videos/",
                           episode_trigger=lambda episode_id: True, name_prefix="pong_eval"
                           )

  total_eval_rewards = 0

  for _ in range(configs.EVAL_LOOPS):

    obs, info = eval_env.reset()
    done = False
    eval_reward = 0

    while not done:
      with torch.no_grad():
        action = qnetwork(torch.from_numpy(np.array(obs, dtype=np.float32)).unsqueeze(0).to(configs.DEVICE)).argmax().item()
      next_obs, reward, terminated, truncation, info = eval_env.step(action)
      obs = next_obs
      eval_reward += float(reward)
      done = terminated or truncation
    print(eval_reward)

    total_eval_rewards += eval_reward

  net_eval_reward = total_eval_rewards / configs.EVAL_LOOPS
  print(total_eval_rewards)
  eval_env.close()
  return net_eval_reward

targetNetwork.eval()
for p in targetNetwork.parameters():
    p.requires_grad = False

def updateTargetNetwork(*, onlineNetwork, targetNetwork , tau):
  with torch.no_grad():
        for target_params, online_params in zip(targetNetwork.parameters(), onlineNetwork.parameters()):
          target_params.copy_(
              tau * online_params.data + (1.0 - tau) * target_params.data
              )

GLOBAL_STEP = 0
EPISODE_RUN = 0

DISCOUNTED_FACTOR = 0.99
EVAL_STEPS = 3000
WANDB_LOGIN_STEP = 2000
tau = 1.0
TRAINING_FREQ = 4
TOTAL_TRAIN_REWARDS = 0
UPDATION_STEP = 4000
SAVE_VIDEO_STEP = 100_000
VERIFY_LEARNING = 100_000

state, _ = env.reset()


for _ in tqdm(range(configs.MAX_EPISODES)):

  EPISODE_RUN += 1

  stateTensor = torch.from_numpy(np.array(state, dtype = np.float32)).unsqueeze(0).to(configs.DEVICE)

  action, epsilon = getAction(state = stateTensor, qnetwork = onlineNetwork, step = GLOBAL_STEP)

  next_state, reward, terminated, truncated, info = env.step(action)

  reward = np.sign(reward)

  done = terminated or truncated

  Memory.append((state, action, reward, next_state, done))

  state = next_state

  TOTAL_TRAIN_REWARDS += np.sign(reward)

  if GLOBAL_STEP%WANDB_LOGIN_STEP == 0:
      run.log({
          "epsilon": epsilon,
          "train-rewards": TOTAL_TRAIN_REWARDS,
          "global_steps": GLOBAL_STEP
    })


  if GLOBAL_STEP >= configs.TRAINING_START_STEPS and GLOBAL_STEP%TRAINING_FREQ == 0:

      states, action_taken, rewards , next_states, dones = get_batches(Memory)

      with torch.no_grad():
        actions_online = onlineNetwork(next_states).argmax(dim = -1, keepdim=True)
        Rt = rewards + DISCOUNTED_FACTOR * targetNetwork.getQvals(next_states, actions_online) * (1.0 - dones.float())

        # online takes actions and target network evalutates that given action

      Vt = onlineNetwork(states).gather(1, action_taken)
      # gradients flow from here and pytorch builds the computational graph.

      loss = F.mse_loss(Vt, Rt.detach())
      OPTIMIZER.zero_grad()
      loss.backward()
      OPTIMIZER.step()
      #SCHEDULER.step()
      if GLOBAL_STEP%UPDATION_STEP == 0:
          updateTargetNetwork(
          onlineNetwork = onlineNetwork,
          targetNetwork = targetNetwork,
          tau = tau
          )

      if GLOBAL_STEP%WANDB_LOGIN_STEP == 0:
        run.log({
                "loss": loss.item(),
                "lr": OPTIMIZER.param_groups[0]["lr"],
                 "episodic-count": EPISODE_RUN})


      if GLOBAL_STEP%EVAL_STEPS==0:
              rec_status = True if GLOBAL_STEP%SAVE_VIDEO_STEP==0 else False
              eval_reward = evaluationLoop(onlineNetwork, rec_status)
              run.log({"Evaluated-reward": eval_reward})
              onlineNetwork.train()

              if rec_status:
                  print(f"Video Saved at {GLOBAL_STEP}")

  if GLOBAL_STEP%VERIFY_LEARNING == 0:
      mean_q = onlineNetwork(states).mean().item()
      max_q = targetNetwork(states).max().item()
      reward_rate = (rewards != 0).float().mean().item()
      print(f'MEAN-Q and MAX-Q VALUES: {mean_q, max_q}')

  GLOBAL_STEP += 1

  if done:
      state, _ = env.reset()
      EPISODE_RUN = 0
      TOTAL_TRAIN_REWARDS = 0

env.close()
run.finish()

torch.save(onlineNetwork.state_dict(), "weights.pt")