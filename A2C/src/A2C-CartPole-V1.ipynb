{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   This is a minimalistic 1-step Advantage Actor–Critic (A2C) RL algorithm implementation.\n",
        "\n",
        "*   The agent collects a full episode of transitions and performs a single batch update afterward.\n",
        "\n",
        "\n",
        "*   The critic uses a 1-step TD target: r + gamma * V(st+1), making it TD(0)-style bootstrapping.\n",
        "*   There is no entropy bonus, no GAE, and no multi-step returns — just the core vanilla A2C logic.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kQdCbE0A2UIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTING LIBRARIES**"
      ],
      "metadata": {
        "id": "datKnudtXc1D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "592W2ND6Vrki"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import deque\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createEnvironment(cfg):\n",
        "  env = gym.make(cfg.game_id, render_mode=\"rgb_array\")\n",
        "  return env"
      ],
      "metadata": {
        "id": "60iIOcw1J2vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WANDB RUN**"
      ],
      "metadata": {
        "id": "HWGd2YkB2l8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wandb_runs(cfg):\n",
        "\n",
        "  wandb.login(key = \"\")\n",
        "  run = wandb.init(\n",
        "    entity=\"ajheshbasnet-kpriet\",\n",
        "    project=\"ddpg\",\n",
        "    name = \"DDPG\",\n",
        "    config=vars(cfg),\n",
        "  )\n",
        "\n",
        "  return run"
      ],
      "metadata": {
        "id": "bS0X6VK-W6C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONFIGURATIONS**"
      ],
      "metadata": {
        "id": "58FAgHd6XgNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class configuration:\n",
        "  game_id = \"CartPole-v1\"\n",
        "  n_rollouts = 100_000\n",
        "  global_steps = 0\n",
        "  eval_loops = 3\n",
        "  critic_lr = 2.5e-4\n",
        "  actor_lr = 2.5e-4\n",
        "  eval_steps = 10_000\n",
        "  record_video = 500_000\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = configuration()"
      ],
      "metadata": {
        "id": "vSCqFzICXY4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envs = createEnvironment(cfg)"
      ],
      "metadata": {
        "id": "QpR1buugKk6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking environment is working or not:)**"
      ],
      "metadata": {
        "id": "REzQ_aBy2u5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envs.reset()[0]"
      ],
      "metadata": {
        "id": "_c-ZAllyz_lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Actor and Critic Netowrk**"
      ],
      "metadata": {
        "id": "Fy_OPT6e3O1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, action_dim):\n",
        "    super().__init__()\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Linear(input_dim, 200),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(200, 200),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(200, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, action_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sequential(x)\n",
        "    return torch.softmax(x, dim = -1)\n",
        "\n",
        "  def log_probs(self, x):\n",
        "        logits = self(x)\n",
        "        dist = Categorical(logits=logits)\n",
        "\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        return log_prob, action"
      ],
      "metadata": {
        "id": "vh8HBQTyOTA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Linear(input_dim, 200),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(200, 200),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(200, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sequential(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "oPaVL6a9O950"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actornet = Actor(envs.reset()[0].shape[0], envs.action_space.n).to(cfg.device)\n",
        "criticnet = Critic(envs.reset()[0].shape[0]).to(cfg.device)"
      ],
      "metadata": {
        "id": "xbTU-sNyPUeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'''Parameters:\n",
        "===========================\n",
        "actor-network :  {sum(p.numel() for p in actornet.parameters())/1e3} k\n",
        "critic-network : {sum(p.numel() for p in criticnet.parameters())/1e3} k\n",
        "===========================\n",
        "      ''')"
      ],
      "metadata": {
        "id": "Mji3FfE4P1HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Loop**"
      ],
      "metadata": {
        "id": "_MWMoXRF3Tqu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14faec0a"
      },
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress SyntaxWarning specifically from the moviepy module\n",
        "warnings.filterwarnings(\"ignore\", category=SyntaxWarning, module=\"moviepy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(actornet, record_video = False):\n",
        "\n",
        "  eval_env = gym.make(id = cfg.game_id, render_mode = 'rgb_array')\n",
        "  if record_video:\n",
        "    video_dir = f\"videos/{cfg.global_steps}\"\n",
        "    eval_env = RecordVideo(eval_env,  video_folder=video_dir, episode_trigger=lambda ep: True)\n",
        "\n",
        "  net_reward = 0\n",
        "  net_step = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for _ in range(cfg.eval_loops):\n",
        "\n",
        "      done = False\n",
        "\n",
        "      episodic_reward = 0\n",
        "      episodic_step = 0\n",
        "      state = eval_env.reset()[0]\n",
        "\n",
        "      while not done:\n",
        "\n",
        "        stateT = torch.tensor(state, dtype=torch.float32, device=cfg.device)\n",
        "        action = actornet(stateT).argmax().item()\n",
        "        nxt_state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "        done = terminated or truncated\n",
        "        state = nxt_state\n",
        "\n",
        "        episodic_reward += float(reward)\n",
        "        episodic_step += 1\n",
        "\n",
        "      net_reward += episodic_reward\n",
        "      net_step  += episodic_step\n",
        "\n",
        "  net_reward = net_reward / cfg.eval_loops\n",
        "  net_step = net_step / cfg.eval_loops\n",
        "\n",
        "  eval_env.close()\n",
        "\n",
        "  return net_reward, net_step"
      ],
      "metadata": {
        "id": "m7xFx7s0WpLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation(actornet, True)"
      ],
      "metadata": {
        "id": "BjjuyVcBzENt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To sample the batches**"
      ],
      "metadata": {
        "id": "Xs-HECi93Wby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "critic_optimizer = torch.optim.AdamW(criticnet.parameters(), lr = cfg.critic_lr)\n",
        "actor_optimizer = torch.optim.AdamW(actornet.parameters(), lr = cfg.actor_lr)"
      ],
      "metadata": {
        "id": "YJwbt4RpR9kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = wandb_runs(cfg)"
      ],
      "metadata": {
        "id": "bY4z2rMkXM4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Heart & Core of the notebook: Vanilla A2C Algorithm's Training Loop**"
      ],
      "metadata": {
        "id": "CU_YjGWi3biG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99"
      ],
      "metadata": {
        "id": "YPxqJG5eSRi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in tqdm(range(cfg.n_rollouts)):\n",
        "\n",
        "  states = []\n",
        "  next_states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  dones = []\n",
        "  log_probs = []\n",
        "\n",
        "  state = envs.reset()[0]\n",
        "\n",
        "  statesT = torch.tensor(state, dtype=torch.float32, device=cfg.device).unsqueeze(0)\n",
        "\n",
        "  done = False\n",
        "\n",
        "  training_reward = 0\n",
        "  training_step = 0\n",
        "\n",
        "  while not done:\n",
        "\n",
        "    log_prob, actionT = actornet.log_probs(statesT)\n",
        "    action = actionT.item()\n",
        "    next_state, reward, terminated, truncated, _ = envs.step(action)\n",
        "    next_stateT = torch.tensor(next_state, dtype=torch.float32, device=cfg.device)\n",
        "    rewardT = torch.tensor(reward, dtype=torch.float32, device=cfg.device)\n",
        "    done = terminated | truncated\n",
        "    doneT = torch.tensor(done, dtype=torch.float32, device=cfg.device)\n",
        "\n",
        "    states.append(statesT.squeeze(0))\n",
        "    next_states.append(next_stateT)\n",
        "    actions.append(actionT.view(-1))\n",
        "    log_probs.append(log_prob.view(-1))\n",
        "    rewards.append(rewardT)\n",
        "    dones.append(doneT)\n",
        "\n",
        "    training_reward += float(reward)\n",
        "    training_step += 1\n",
        "    statesT = next_stateT.unsqueeze(0)\n",
        "\n",
        "    cfg.global_steps += 1\n",
        "    runs.log({\"global-steps\": cfg.global_steps})\n",
        "    actornet.eval()\n",
        "    if cfg.global_steps%cfg.eval_steps == 0 and cfg.global_steps>0:\n",
        "      rec = True if cfg.global_steps%cfg.record_video == 0 else False\n",
        "      net_reward, net_step = evaluation(actornet, rec)\n",
        "      runs.log({\"evaluation-reward\": net_reward, \"eval-steps\": net_step})\n",
        "      actornet.train()\n",
        "\n",
        "  all_states = torch.stack(states)\n",
        "  all_next_states = torch.stack(next_states)\n",
        "  all_actions = torch.stack(actions)\n",
        "  all_log_probs = torch.stack(log_probs).view(-1)\n",
        "  all_rewards = torch.stack(rewards)\n",
        "  all_dones = torch.stack(dones)\n",
        "\n",
        "  # all_Gt = []\n",
        "  # Gt = 0\n",
        "\n",
        "  # for r in reversed(all_rewards):\n",
        "  #   Gt = r + gamma*Gt\n",
        "  #   all_Gt.insert(0, Gt)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    next_target = all_rewards + (1 - all_dones) * gamma * criticnet(all_next_states).squeeze(-1)\n",
        "\n",
        "  values = criticnet(all_states).squeeze(-1)\n",
        "\n",
        "  Advantages = next_target - values\n",
        "\n",
        "  Advantages = (Advantages - Advantages.mean()) / (Advantages.std() + 1e-9)\n",
        "\n",
        "  actorloss = - (all_log_probs * Advantages.detach()).mean()\n",
        "\n",
        "  criticloss = torch.nn.functional.mse_loss(values, next_target.detach())\n",
        "\n",
        "  runs.log({\"actor-loss\": actorloss.item(), \"critic-loss\": criticloss.item()})\n",
        "\n",
        "  actor_optimizer.zero_grad()\n",
        "  actorloss.backward()\n",
        "  actor_optimizer.step()\n",
        "\n",
        "  critic_optimizer.zero_grad()\n",
        "  criticloss.backward()\n",
        "  critic_optimizer.step()\n",
        "\n",
        "  runs.log({\"training_reward\" : training_reward, \"advantages\": Advantages.mean().item(), \"training-step\": training_step})"
      ],
      "metadata": {
        "id": "AnYrCFL0XspN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Save the Model Weights**"
      ],
      "metadata": {
        "id": "DVo3R2Xqzc6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(actornet.state_dict(), \"actor.pt\")"
      ],
      "metadata": {
        "id": "EE2mwd1dzets"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}