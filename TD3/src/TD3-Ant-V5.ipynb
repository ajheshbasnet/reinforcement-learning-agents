{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Run the below cell to download the MuJoCo dependencies"
      ],
      "metadata": {
        "id": "kQdCbE0A2UIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gymnasium[mujoco] mujoco"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NSQT9cmkLW_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lPpgmu6bbnCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTING LIBRARIES**"
      ],
      "metadata": {
        "id": "datKnudtXc1D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "592W2ND6Vrki"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import deque\n",
        "import wandb\n",
        "\n",
        "import gymnasium as gym\n",
        "# from gymnasium.vector import SyncVectorEnv\n",
        "from gymnasium.wrappers import RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MUJOCO_GL\"] = \"egl\""
      ],
      "metadata": {
        "id": "C_JNsAjzZ5T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_environment(cfgs, eval = False):\n",
        "  env = gym.make( id=cfgs.id , render_mode=\"rgb_array\", max_episode_steps=cfg.max_steps)\n",
        "  return env"
      ],
      "metadata": {
        "id": "r7ghF0Mvsxjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WANDB RUN**"
      ],
      "metadata": {
        "id": "HWGd2YkB2l8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wandb_runs(cfg):\n",
        "\n",
        "  wandb.login(key = \"\")\n",
        "  run = wandb.init(\n",
        "    entity=\"ajheshbasnet-kpriet\",\n",
        "    project=\"ddpg\",\n",
        "    name = \"DDPG\",\n",
        "    config=vars(cfg),\n",
        "  )\n",
        "\n",
        "  return run"
      ],
      "metadata": {
        "id": "bS0X6VK-W6C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONFIGURATIONS**"
      ],
      "metadata": {
        "id": "58FAgHd6XgNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class configuration:\n",
        "  id = \"Ant-v5\"\n",
        "  n_rollouts = 100_000\n",
        "  max_steps = 1000\n",
        "  eval_steps = 10_000\n",
        "  global_steps = 0\n",
        "  buffer_size = 800_000\n",
        "  eval_loops = 3\n",
        "  batch_size = 512\n",
        "  wandb_log_steps = 50\n",
        "  start_training = 50_000\n",
        "  training_step = 2\n",
        "  actor_freq = 2\n",
        "  critic_lr = 2.5e-4\n",
        "  actor_lr = 2.5e-4\n",
        "  record_video = 500_000\n",
        "  eval_max_steps = 800\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = configuration()"
      ],
      "metadata": {
        "id": "vSCqFzICXY4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envs = create_environment(cfg)"
      ],
      "metadata": {
        "id": "QpR1buugKk6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Actor and Critic Netowrk**"
      ],
      "metadata": {
        "id": "Fy_OPT6e3O1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, action_dim):\n",
        "    super().__init__()\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Linear(input_dim, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, action_dim),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sequential(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "vh8HBQTyOTA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Linear(input_dim, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    x = torch.cat([state, action], dim = 1)\n",
        "    x = self.sequential(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "oPaVL6a9O950"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(envs.observation_space,\"\\t\", envs.action_space,)"
      ],
      "metadata": {
        "id": "or3uYM8APYcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actornet = Actor(envs.observation_space.shape[0], envs.action_space.shape[0]).to(cfg.device)  #type: ignore\n",
        "criticnet1 = Critic(envs.observation_space.shape[0]+envs.action_space.shape[0]).to(cfg.device)  #type: ignore\n",
        "criticnet2 = Critic(envs.observation_space.shape[0]+envs.action_space.shape[0]).to(cfg.device)  #type: ignore\n",
        "\n",
        "TargetActor = Actor(envs.observation_space.shape[0], envs.action_space.shape[0]).to(cfg.device) #type: ignore\n",
        "TargetCritic1 = Critic(envs.observation_space.shape[0]+envs.action_space.shape[0]).to(cfg.device) #type: ignore\n",
        "TargetCritic2 = Critic(envs.observation_space.shape[0]+envs.action_space.shape[0]).to(cfg.device) #type: ignore\n",
        "\n",
        "TargetActor.load_state_dict(actornet.state_dict())\n",
        "TargetCritic1.load_state_dict(criticnet1.state_dict())\n",
        "TargetCritic2.load_state_dict(criticnet2.state_dict())"
      ],
      "metadata": {
        "id": "xbTU-sNyPUeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'''Parameters:\n",
        "=================================================================\n",
        "actor-network     : {sum(p.numel() for p in actornet.parameters())/1e3} k\n",
        "critic-network(s) : {sum(p.numel() for p in criticnet1.parameters())/ 1e3} k + {sum(p.numel() for p in criticnet2.parameters())/ 1e3} k\n",
        "=================================================================\n",
        "      ''')"
      ],
      "metadata": {
        "id": "Mji3FfE4P1HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Loop**"
      ],
      "metadata": {
        "id": "_MWMoXRF3Tqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(actornet, record_video = False):\n",
        "\n",
        "  eval_env = gym.make(id = cfg.id, render_mode = 'rgb_array' ,max_episode_steps=cfg.eval_max_steps)\n",
        "  if record_video:\n",
        "    video_dir = f\"videos/{int(time.time())}\"\n",
        "    eval_env = RecordVideo(eval_env,  video_folder=video_dir, episode_trigger=lambda ep: True)\n",
        "\n",
        "  net_reward = 0\n",
        "  net_step = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for _ in range(cfg.eval_loops):\n",
        "\n",
        "      done = False\n",
        "\n",
        "      episodic_reward = 0\n",
        "      episodic_step = 0\n",
        "      state = eval_env.reset()[0]\n",
        "\n",
        "      while not done:\n",
        "\n",
        "        stateT = torch.tensor(state, dtype=torch.float32, device=cfg.device)\n",
        "        action = np.array(actornet(stateT).cpu().numpy())\n",
        "        nxt_state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "        done = terminated or truncated\n",
        "        state = nxt_state\n",
        "\n",
        "        episodic_reward += float(reward)\n",
        "        episodic_step += 1\n",
        "\n",
        "      print(episodic_reward)\n",
        "      net_reward += episodic_reward\n",
        "      net_step  += episodic_step\n",
        "\n",
        "  net_reward = net_reward / cfg.eval_loops\n",
        "  net_step = net_step / cfg.eval_loops\n",
        "\n",
        "  eval_env.close()\n",
        "\n",
        "  return net_reward, net_step"
      ],
      "metadata": {
        "id": "m7xFx7s0WpLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(actornet, True)"
      ],
      "metadata": {
        "id": "z2hkd4Aw6SL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To sample the batches**"
      ],
      "metadata": {
        "id": "Xs-HECi93Wby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(memory, batch_size):\n",
        "    batches = random.sample(memory, batch_size)\n",
        "    state, action, reward, next_state, done = zip(*batches)\n",
        "\n",
        "    state = torch.stack(state).float().to(cfg.device)\n",
        "    action = torch.stack(action).float().to(cfg.device)\n",
        "    reward = torch.stack(reward).float().to(cfg.device)\n",
        "    next_state = torch.stack(next_state).float().to(cfg.device)\n",
        "    done = torch.stack(done).float().to(cfg.device)  # float for TD computation\n",
        "\n",
        "    return state, action, reward.view(-1, 1), next_state, done.view(-1, 1)"
      ],
      "metadata": {
        "id": "NC2tSGnliXeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **REPLAY MEMORY**"
      ],
      "metadata": {
        "id": "pKROh-L3Z4_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = deque(maxlen = cfg.buffer_size)\n",
        "action_sigma = 0.1\n",
        "tau = 0.001\n",
        "gamma = 0.99\n",
        "noise_clip = 0.5\n",
        "policy_noise = 0.2\n",
        "global_step = cfg.global_steps"
      ],
      "metadata": {
        "id": "h-lQZ4unSgB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "critic_optimizer1 = torch.optim.AdamW(criticnet1.parameters(), lr = cfg.critic_lr)\n",
        "critic_optimizer2 = torch.optim.AdamW(criticnet2.parameters(), lr = cfg.critic_lr)\n",
        "actor_optimizer = torch.optim.AdamW(actornet.parameters(), lr = cfg.actor_lr)"
      ],
      "metadata": {
        "id": "YJwbt4RpR9kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**W&B RUNS TO LOG THE METRICS**"
      ],
      "metadata": {
        "id": "r-aIg5JqZ9zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runs = wandb_runs(cfg)"
      ],
      "metadata": {
        "id": "bY4z2rMkXM4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Heart & Core of the notebook: DDPG Algorithm's Training Loop**"
      ],
      "metadata": {
        "id": "CU_YjGWi3biG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for _ in tqdm(range(cfg.n_rollouts)):\n",
        "\n",
        "  states = envs.reset()[0]\n",
        "\n",
        "  statesT= torch.tensor(states, dtype=torch.float32, device = cfg.device)\n",
        "\n",
        "  training_rewards = 0\n",
        "\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "\n",
        "    with torch.no_grad():\n",
        "      action = actornet(statesT).view(-1)\n",
        "\n",
        "    action_noise = torch.randn_like(action)\n",
        "\n",
        "    action = (action + action_sigma * action_noise)\n",
        "\n",
        "    action = torch.clamp(action, -1.0, 1.0).cpu().numpy()\n",
        "\n",
        "    next_states, rewards, terminated, truncated, _ =  envs.step(action)\n",
        "\n",
        "    done = terminated | truncated\n",
        "\n",
        "    next_statesT = torch.tensor(next_states, dtype=torch.float32, device = cfg.device)\n",
        "\n",
        "    actionT = torch.tensor(action, dtype=torch.float32, device = cfg.device)\n",
        "\n",
        "    rewardsT = torch.tensor(rewards, dtype=torch.float32, device=cfg.device)\n",
        "\n",
        "    training_rewards += float(rewards)\n",
        "\n",
        "    doneT = torch.tensor(done, dtype=torch.bool, device = cfg.device)\n",
        "\n",
        "    replay_buffer.append((statesT.detach(), actionT.detach(), rewardsT.detach(), next_statesT.detach(), doneT.detach()))\n",
        "\n",
        "    statesT = next_statesT\n",
        "\n",
        "    if cfg.global_steps% cfg.training_step == 0 and len(replay_buffer)>cfg.start_training:\n",
        "        # Sample batch\n",
        "        states_b, action_b, reward_b, next_states_b, dones_b = get_batches(replay_buffer, cfg.batch_size)\n",
        "\n",
        "        # Target Q\n",
        "        with torch.no_grad():\n",
        "\n",
        "          next_action_ = TargetActor(next_states_b)\n",
        "          noise_next_action = torch.clamp(torch.randn_like(next_action_) * policy_noise, -noise_clip, +noise_clip)\n",
        "          next_action = next_action_ + noise_next_action\n",
        "          next_action = torch.clamp(next_action, -1.0, 1.0)\n",
        "\n",
        "          target_next_q1 = TargetCritic1(next_states_b, next_action)\n",
        "          target_next_q2 = TargetCritic2(next_states_b, next_action)\n",
        "          target_q = reward_b + gamma * torch.min(target_next_q1, target_next_q2) * (1 - dones_b.float())\n",
        "\n",
        "        # Current critic Q\n",
        "        current_q1 = criticnet1(states_b, action_b)\n",
        "        current_q2 = criticnet2(states_b, action_b)\n",
        "\n",
        "        # Critic loss\n",
        "        critic_loss1 = torch.nn.functional.mse_loss(current_q1, target_q)\n",
        "        critic_loss2 = torch.nn.functional.mse_loss(current_q2, target_q)\n",
        "\n",
        "        # Optimize critic1\n",
        "        critic_optimizer1.zero_grad()\n",
        "        critic_loss1.backward()\n",
        "        critic_grad_norm1 = torch.nn.utils.clip_grad_norm_(criticnet1.parameters(), max_norm=1.0)\n",
        "        critic_optimizer1.step()\n",
        "\n",
        "        # Optimize critic2\n",
        "        critic_optimizer2.zero_grad()\n",
        "        critic_loss2.backward()\n",
        "        critic_grad_norm2 = torch.nn.utils.clip_grad_norm_(criticnet2.parameters(), max_norm=1.0)\n",
        "        critic_optimizer2.step()\n",
        "\n",
        "        # Actor loss (use current actor)\n",
        "        # Clone states_b to create an independent computational graph for actor update\n",
        "        states_b_actor = states_b.clone()\n",
        "        actor_actions = actornet(states_b_actor) # Renamed to avoid shadowing action_b from get_batches\n",
        "\n",
        "        min_q = torch.min(criticnet1(states_b_actor, actor_actions), criticnet2(states_b_actor, actor_actions))\n",
        "\n",
        "        actor_loss = -min_q.mean()\n",
        "\n",
        "        if cfg.global_steps%cfg.actor_freq==0:\n",
        "\n",
        "          # Optimize actor\n",
        "          actor_optimizer.zero_grad()\n",
        "          actor_loss.backward()\n",
        "          actor_grad_norm = torch.nn.utils.clip_grad_norm_(actornet.parameters(), max_norm=1.0)\n",
        "          actor_optimizer.step()\n",
        "\n",
        "          for target_param, param in zip(TargetActor.parameters(), actornet.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        advantages = (target_q - torch.min(current_q1, current_q2)).detach().mean()\n",
        "\n",
        "        runs.log({ \"actor-loss\": actor_loss.item(), \"critic-loss1\": critic_loss1.item(), \"critic-loss2\": critic_loss2.item(),\"advantages\": advantages.item()})\n",
        "\n",
        "        # Soft update targets\n",
        "        for target_param, param in zip(TargetCritic1.parameters(), criticnet1.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        for target_param, param in zip(TargetCritic2.parameters(), criticnet2.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        if cfg.global_steps%cfg.eval_steps==0 and cfg.global_steps>0:\n",
        "\n",
        "            rec = True if cfg.global_steps%cfg.record_video==0 else False\n",
        "            eval_reward, eval_steps = evaluation(actornet, rec)\n",
        "            runs.log(\n",
        "                {\n",
        "                    \"eval-reward\": eval_reward,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    runs.log({\"training-reward\": training_rewards, \"global-steps\": cfg.global_steps, \"memory\": len(replay_buffer)})\n",
        "\n",
        "    cfg.global_steps += 1\n",
        "\n",
        "envs.close()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "AnYrCFL0XspN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}