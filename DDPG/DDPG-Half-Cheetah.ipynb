{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Run the below cell to download the MuJoCo dependencies"
      ],
      "metadata": {
        "id": "kQdCbE0A2UIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gymnasium[mujoco] mujoco"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NSQT9cmkLW_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lPpgmu6bbnCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTING LIBRARIES**"
      ],
      "metadata": {
        "id": "datKnudtXc1D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "592W2ND6Vrki"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import deque\n",
        "import wandb\n",
        "\n",
        "import gymnasium as gym\n",
        "# from gymnasium.vector import SyncVectorEnv\n",
        "from gymnasium.wrappers import RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"MUJOCO_GL\"] = \"egl\""
      ],
      "metadata": {
        "id": "C_JNsAjzZ5T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_environment(cfgs, eval = False):\n",
        "\n",
        "#   def _init():\n",
        "#       env = gym.make( id=cfgs.id , render_mode=\"rgb_array\", max_episode_steps=cfg.max_steps)\n",
        "#       return env\n",
        "\n",
        "#   return _init"
      ],
      "metadata": {
        "id": "60iIOcw1J2vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_environment(cfgs, eval = False):\n",
        "  env = gym.make( id=cfgs.id , render_mode=\"rgb_array\", max_episode_steps=cfg.max_steps)\n",
        "  return env"
      ],
      "metadata": {
        "id": "r7ghF0Mvsxjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WANDB RUN**"
      ],
      "metadata": {
        "id": "HWGd2YkB2l8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wandb_runs(cfg):\n",
        "\n",
        "  wandb.login(key = \"\")\n",
        "  run = wandb.init(\n",
        "    entity=\"ajheshbasnet-kpriet\",\n",
        "    project=\"ddpg\",\n",
        "    name = \"DDPG\",\n",
        "    config=vars(cfg),\n",
        "  )\n",
        "\n",
        "  return run"
      ],
      "metadata": {
        "id": "bS0X6VK-W6C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONFIGURATIONS**"
      ],
      "metadata": {
        "id": "58FAgHd6XgNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class configuration:\n",
        "  id = \"HalfCheetah-v5\"\n",
        "  n_rollouts = 100_000\n",
        "  max_steps = 1000\n",
        "  eval_steps = 10_000\n",
        "  global_steps = 0\n",
        "  buffer_size = 800_000\n",
        "  eval_loops = 3\n",
        "  batch_size = 512\n",
        "  wandb_log_steps = 50\n",
        "  start_training = 80_000\n",
        "  trainng_step = 1\n",
        "  critic_lr = 5e-4\n",
        "  actor_lr = 5e-4\n",
        "  record_video = 500_000\n",
        "  eval_max_steps = 800\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = configuration()"
      ],
      "metadata": {
        "id": "vSCqFzICXY4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SyncVectorEnv so that we can run the n-environments parrallelly and utilize the GPUs because single environment is wayy poor**"
      ],
      "metadata": {
        "id": "be7coiwP20R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# envs = SyncVectorEnv([create_environment(cfg) for _ in range(cfg.n_envs)])\n",
        "\n",
        "envs = create_environment(cfg)"
      ],
      "metadata": {
        "id": "QpR1buugKk6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking environment is working or not:)**"
      ],
      "metadata": {
        "id": "REzQ_aBy2u5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envs.reset()[0]"
      ],
      "metadata": {
        "id": "ScTeeDVHU-KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Actor and Critic Netowrk**"
      ],
      "metadata": {
        "id": "Fy_OPT6e3O1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, action_dim):\n",
        "    super().__init__()\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Linear(input_dim, 400),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(400, 300),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(300, action_dim),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sequential(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "vh8HBQTyOTA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Linear(input_dim, 400),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(400, 300),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(300, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    x = torch.cat([state, action], dim = 1)\n",
        "    x = self.sequential(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "oPaVL6a9O950"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(envs.observation_space,\"\\t\", envs.action_space,)"
      ],
      "metadata": {
        "id": "or3uYM8APYcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DXVHtp-latXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actornet = Actor(envs.observation_space.shape[0], envs.action_space.shape[0]).to(cfg.device)  #type: ignore\n",
        "criticnet = Critic(envs.observation_space.shape[0]+envs.action_space.shape[0]).to(cfg.device)  #type: ignore\n",
        "\n",
        "TargetActor = Actor(envs.observation_space.shape[0], envs.action_space.shape[0]).to(cfg.device) #type: ignore\n",
        "TargetCritic = Critic(envs.observation_space.shape[0]+envs.action_space.shape[0]).to(cfg.device) #type: ignore\n",
        "\n",
        "TargetActor.load_state_dict(actornet.state_dict())\n",
        "TargetCritic.load_state_dict(criticnet.state_dict())"
      ],
      "metadata": {
        "id": "xbTU-sNyPUeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'''Parameters:\n",
        "===========================\n",
        "actor-network :  {sum(p.numel() for p in actornet.parameters())/1e3} k\n",
        "critic-network : {sum(p.numel() for p in criticnet.parameters())/ 1e3} k\n",
        "===========================\n",
        "      ''')"
      ],
      "metadata": {
        "id": "Mji3FfE4P1HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Loop**"
      ],
      "metadata": {
        "id": "_MWMoXRF3Tqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(actornet, record_video = False):\n",
        "\n",
        "  eval_env = gym.make(id = cfg.id, render_mode = 'rgb_array' ,max_episode_steps=cfg.eval_max_steps)\n",
        "  if record_video:\n",
        "    video_dir = f\"videos/{int(time.time())}\"\n",
        "    eval_env = RecordVideo(eval_env,  video_folder=video_dir, episode_trigger=lambda ep: True)\n",
        "\n",
        "  net_reward = 0\n",
        "  net_step = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for _ in range(cfg.eval_loops):\n",
        "\n",
        "      done = False\n",
        "\n",
        "      episodic_reward = 0\n",
        "      episodic_step = 0\n",
        "      state = eval_env.reset()[0]\n",
        "\n",
        "      while not done:\n",
        "\n",
        "        stateT = torch.tensor(state, dtype=torch.float32, device=cfg.device)\n",
        "        action = np.array(actornet(stateT).cpu().numpy())\n",
        "        nxt_state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "        done = terminated or truncated\n",
        "        state = nxt_state\n",
        "\n",
        "        episodic_reward += float(reward)\n",
        "        episodic_step += 1\n",
        "\n",
        "      net_reward += episodic_reward\n",
        "      net_step  += episodic_step\n",
        "\n",
        "  net_reward = net_reward / cfg.eval_loops\n",
        "  net_step = net_step / cfg.eval_loops\n",
        "\n",
        "  eval_env.close()\n",
        "\n",
        "  return net_reward, net_step"
      ],
      "metadata": {
        "id": "m7xFx7s0WpLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(actornet, False)"
      ],
      "metadata": {
        "id": "z2hkd4Aw6SL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To sample the batches**"
      ],
      "metadata": {
        "id": "Xs-HECi93Wby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(memory, batch_size):\n",
        "    batches = random.sample(memory, batch_size)\n",
        "    state, action, reward, next_state, done = zip(*batches)\n",
        "\n",
        "    state = torch.stack(state).float().to(cfg.device)\n",
        "    action = torch.stack(action).float().to(cfg.device)\n",
        "    reward = torch.stack(reward).float().to(cfg.device)\n",
        "    next_state = torch.stack(next_state).float().to(cfg.device)\n",
        "    done = torch.stack(done).float().to(cfg.device)  # float for TD computation\n",
        "\n",
        "    return state, action, reward.view(-1, 1), next_state, done.view(-1, 1)"
      ],
      "metadata": {
        "id": "NC2tSGnliXeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **REPLAY MEMORY**"
      ],
      "metadata": {
        "id": "pKROh-L3Z4_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = deque(maxlen = cfg.buffer_size)\n",
        "action_sigma = 0.1\n",
        "tau = 0.001\n",
        "gamma = 0.99\n",
        "global_step = cfg.global_steps"
      ],
      "metadata": {
        "id": "h-lQZ4unSgB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "critic_optimizer = torch.optim.AdamW(criticnet.parameters(), lr = cfg.critic_lr)\n",
        "actor_optimizer = torch.optim.AdamW(actornet.parameters(), lr = cfg.actor_lr)"
      ],
      "metadata": {
        "id": "YJwbt4RpR9kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**W&B RUNS TO LOG THE METRICS**"
      ],
      "metadata": {
        "id": "r-aIg5JqZ9zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runs = wandb_runs(cfg)"
      ],
      "metadata": {
        "id": "bY4z2rMkXM4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Heart & Core of the notebook: DDPG Algorithm's Training Loop**"
      ],
      "metadata": {
        "id": "CU_YjGWi3biG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for _ in tqdm(range(cfg.n_rollouts)):\n",
        "\n",
        "  states = envs.reset()[0]\n",
        "\n",
        "  statesT= torch.tensor(states, dtype=torch.float32, device = cfg.device)\n",
        "\n",
        "  training_rewards = 0\n",
        "\n",
        "  for _ in range(cfg.max_steps):\n",
        "\n",
        "    with torch.no_grad():\n",
        "      action = actornet(statesT).view(-1)\n",
        "\n",
        "    action = (action + action_sigma * torch.randn(6).to(cfg.device)).cpu().numpy()\n",
        "\n",
        "    next_states, rewards, terminated, truncated, _ =  envs.step(action)\n",
        "\n",
        "    done = terminated | truncated\n",
        "\n",
        "    next_statesT = torch.tensor(next_states, dtype=torch.float32, device = cfg.device)\n",
        "\n",
        "    actionT = torch.tensor(action, dtype=torch.float32, device = cfg.device)\n",
        "\n",
        "    rewardsT = torch.tensor(rewards, dtype=torch.float32, device=cfg.device)\n",
        "\n",
        "    training_rewards += float(rewards)\n",
        "\n",
        "    doneT = torch.tensor(done, dtype=torch.bool, device = cfg.device)\n",
        "\n",
        "    replay_buffer.append((statesT, actionT, rewardsT, next_statesT, doneT))\n",
        "\n",
        "    if (global_step + 1) % cfg.trainng_step == 0 and len(replay_buffer)>cfg.start_training:\n",
        "      # Sample batch\n",
        "      states_b, action_b, reward_b, next_states_b, dones_b = get_batches(replay_buffer, cfg.batch_size)\n",
        "\n",
        "      # Target Q\n",
        "      with torch.no_grad():\n",
        "        next_action = TargetActor(next_states_b)\n",
        "        target_next_q = TargetCritic(next_states_b, next_action)\n",
        "        target_q = reward_b + gamma * target_next_q * (1 - dones_b.float())\n",
        "\n",
        "      # Current critic Q\n",
        "      current_q = criticnet(states_b, action_b)\n",
        "\n",
        "      # Critic loss\n",
        "      critic_loss = torch.nn.functional.mse_loss(current_q, target_q)\n",
        "\n",
        "      # Optimize critic\n",
        "      critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      critic_grad_norm = torch.nn.utils.clip_grad_norm_(criticnet.parameters(), max_norm=1.0)\n",
        "      critic_optimizer.step()\n",
        "\n",
        "      # Actor loss (use current actor)\n",
        "      # Clone states_b to create an independent computational graph for actor update\n",
        "      states_b_actor = states_b.clone()\n",
        "      actor_actions = actornet(states_b_actor) # Renamed to avoid shadowing action_b from get_batches\n",
        "      actor_loss = -criticnet(states_b_actor, actor_actions).mean()\n",
        "\n",
        "      for p in criticnet.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "      # Optimize actor\n",
        "      actor_optimizer.zero_grad()\n",
        "      actor_loss.backward()\n",
        "      critic_grad_norm = torch.nn.utils.clip_grad_norm_(criticnet.parameters(), max_norm=1.0)\n",
        "      actor_optimizer.step()\n",
        "\n",
        "      for p in criticnet.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "\n",
        "      advantages = (target_q - current_q).detach().mean()\n",
        "\n",
        "      runs.log(\n",
        "          {\n",
        "              \"actor-loss\": actor_loss.item(),\n",
        "              \"critic-loss\": critic_loss.item(),\n",
        "              \"advantages\": advantages.item()\n",
        "          }\n",
        "      )\n",
        "\n",
        "      # Soft update targets\n",
        "      for target_param, param in zip(TargetCritic.parameters(), criticnet.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "      for target_param, param in zip(TargetActor.parameters(), actornet.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "      if global_step%cfg.eval_steps==0 and global_step>0:\n",
        "\n",
        "        rec = True if global_step%cfg.record_video==0 else False\n",
        "        eval_reward, eval_steps = evaluation(actornet, rec)\n",
        "        runs.log(\n",
        "            {\n",
        "                \"eval-reward\": eval_reward,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    runs.log({\"training-reward\": training_rewards, \"global-steps\": global_step, \"memory\": len(replay_buffer)})\n",
        "\n",
        "    statesT = next_statesT\n",
        "    global_step += 1\n",
        "\n",
        "envs.close()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "AnYrCFL0XspN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the below cell to save the video from the latest Actor Network**"
      ],
      "metadata": {
        "id": "BL52dUql3pwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(actornet, True)"
      ],
      "metadata": {
        "id": "2aDI241Wi0nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **                         **END****"
      ],
      "metadata": {
        "id": "czl6Q_OzaZsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "53fXpTWeacMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}