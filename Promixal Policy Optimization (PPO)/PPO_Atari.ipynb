{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbQDcy-tLQtn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ale_py\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation, RecordVideo\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(\n",
        "    key = \" \"\n",
        ")"
      ],
      "metadata": {
        "id": "U3fPZ5BWmnqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7dc2e5-d9e9-4a42-9afd-745284802562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33majheshbasnet\u001b[0m (\u001b[33majheshbasnet-kpriet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class configs:\n",
        "\n",
        "  game_id = \"RiverraidNoFrameskip-v4\"\n",
        "  max_step = 5_000\n",
        "  stack_size = 4\n",
        "  n_episodes = 100_000\n",
        "  policy_lr = 3e-3\n",
        "  value_lr = 2.5e-3\n",
        "  discount_factor = 0.99\n",
        "  epsilon = 0.2\n",
        "  oldPolicy_updationStep = 2_000\n",
        "  eval_steps = 5000\n",
        "  cam_counter = 40_000\n",
        "  eval_loops = 3\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = configs()"
      ],
      "metadata": {
        "id": "W_2gx0H1La1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_run(configs):\n",
        "    return wandb.init(\n",
        "    name = \"ppo\",\n",
        "    project=\"ppo\",\n",
        "    # Track hyperparameters and run metadata.\n",
        "    config=vars(configs)\n",
        "    )"
      ],
      "metadata": {
        "id": "Pa7dYp9QmSFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createEnvironment(cfg):\n",
        "\n",
        "  env = gym.make(cfg.game_id, frameskip = 1, full_action_space=False, render_mode=\"rgb_array\", max_episode_steps=configs.max_step)\n",
        "\n",
        "  env = AtariPreprocessing(env, frame_skip=4, grayscale_obs=True, screen_size = 84)\n",
        "  # \"scale_obs\" means the pixels are scaled/normalised from 0 to 1 else it's in uint8 number--> keeping it False because to store it the float32 takes way huge memory so the training will be too much slow around 11s/iteration. Hence do it during the run time only.\n",
        "\n",
        "  env = FrameStackObservation(env, cfg.stack_size)\n",
        "  # it gives [frame(t-3), frame(t-2), frame(t-1), frame(t)] NOT [frame(t), frame(t+1), frame(t+2), frame(t+3)]\n",
        "\n",
        "  # during env.reset() it gives obs = stack of [obs, obs, obs, obs] which is the same frame during the first time\n",
        "  # so after the 1st action the stack becomes [f0, f0, f0, f1] and after another action it becomes [f0, f0, f1, f2] and so on.\n",
        "\n",
        "  return env"
      ],
      "metadata": {
        "id": "VdSiiOSuO1IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = createEnvironment(cfg)"
      ],
      "metadata": {
        "id": "0MddU5CbPArZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, action_space):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(cfg.stack_size, 32, kernel_size=5, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.ffnn = nn.Sequential(\n",
        "        nn.Linear(1024, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, action_space)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x/255.)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.ffnn(x)\n",
        "    return x\n",
        "\n",
        "  def log_probs(self, x):\n",
        "    action_probs = torch.nn.functional.softmax(self(x), dim = -1)\n",
        "    action_idx = torch.multinomial(action_probs, 1)\n",
        "\n",
        "    log_prob = torch.gather(action_probs, -1, action_idx).log()\n",
        "    return action_idx, log_prob"
      ],
      "metadata": {
        "id": "OQVJHZiOMTQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Value_Network(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(cfg.stack_size, 32, kernel_size=5, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 64, kernel_size=4, stride=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.ffnn = nn.Sequential(\n",
        "        nn.Linear(1024, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x/255.)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.ffnn(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Zl-j7x3OaCrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "currentPolicy = PolicyNetwork(env.action_space.n).to(cfg.device)\n",
        "oldPolicy = PolicyNetwork(env.action_space.n).to(cfg.device)\n",
        "ValueNetwork = Value_Network().to(cfg.device)\n",
        "\n",
        "oldPolicy.load_state_dict(currentPolicy.state_dict())\n",
        "oldPolicy.eval()"
      ],
      "metadata": {
        "id": "sGKFq3rKPsqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36982d4-66c9-4b1c-82b3-80722372be92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PolicyNetwork(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(4, 32, kernel_size=(5, 5), stride=(4, 4))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(3, 3))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (ffnn): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=64, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=64, out_features=18, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_optimizer = torch.optim.Adam(currentPolicy.parameters(), cfg.policy_lr)\n",
        "value_optimizer = torch.optim.Adam(ValueNetwork.parameters(), cfg.value_lr)"
      ],
      "metadata": {
        "id": "FOb3fNgQWImg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'''\n",
        "=======================================================================\n",
        "> Actor-Net:  {sum(p.numel() for p in currentPolicy.parameters())/1e3} k\n",
        "> Policy-Net: {sum(p.numel() for p in ValueNetwork.parameters())/1e3} k\n",
        "-----------------------------------------------------------------------\n",
        "> {cfg.device.upper()} is being used\n",
        "=======================================================================\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJPCgBNN3pCS",
        "outputId": "240454db-4522-4c64-ab24-192a5915397a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================================\n",
            "> Actor-Net:  143.922 k\n",
            "> Policy-Net: 142.817 k\n",
            "-----------------------------------------------------------------------\n",
            "> CUDA is being used\n",
            "=======================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluationLoop(policynetwork, recordVideo = False):\n",
        "\n",
        "  eval_env = createEnvironment(cfg)\n",
        "\n",
        "  if recordVideo:\n",
        "    eval_env = RecordVideo(\n",
        "                           eval_env, video_folder=\"videos/\",\n",
        "                           episode_trigger=lambda episode_id: True, name_prefix=\"ppo\"\n",
        "                           )\n",
        "\n",
        "  total_eval_rewards = 0\n",
        "  total_eval_steps = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for _ in range(configs.eval_loops):\n",
        "\n",
        "      obs, _ = eval_env.reset()\n",
        "      done = False\n",
        "\n",
        "      ep_reward = 0.0\n",
        "      ep_step = 0\n",
        "\n",
        "      while not done:\n",
        "\n",
        "        action = policynetwork.log_probs(torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(cfg.device))[0]\n",
        "        next_obs,reward, terminated, truncated, _ =  eval_env.step(action.item())\n",
        "        obs = next_obs\n",
        "        ep_reward += float(reward)\n",
        "        done = terminated or truncated\n",
        "        ep_step += 1\n",
        "\n",
        "      total_eval_rewards += ep_reward\n",
        "      total_eval_steps += ep_step\n",
        "\n",
        "    total_eval_rewards = total_eval_rewards / cfg.eval_loops\n",
        "    total_eval_steps = int(total_eval_steps / cfg.eval_loops)\n",
        "\n",
        "  eval_env.close()\n",
        "\n",
        "  return total_eval_rewards, total_eval_steps"
      ],
      "metadata": {
        "id": "tjaXt0gIRQgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = create_run(cfg)\n",
        "\n",
        "global_step = 0\n",
        "wandblogin_step = 500\n",
        "\n",
        "for steps in tqdm(range(cfg.n_episodes)):\n",
        "\n",
        "  all_states_ = []\n",
        "  all_log_probs_ = []\n",
        "  all_rewards_ = []\n",
        "\n",
        "  episodic_step = 0\n",
        "\n",
        "  done = False\n",
        "\n",
        "  state, _ = env.reset()\n",
        "\n",
        "  training_reward = 0.0\n",
        "\n",
        "  while not done:\n",
        "\n",
        "    stateTensor = torch.tensor(state, dtype=torch.float32).to(cfg.device)\n",
        "\n",
        "    action, log_prob = currentPolicy.log_probs(stateTensor.unsqueeze(0))\n",
        "\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    reward_tensor = torch.tensor(reward, dtype=torch.float).to(cfg.device)\n",
        "\n",
        "    all_states_.append(stateTensor)\n",
        "    all_log_probs_.append(log_prob)\n",
        "    all_rewards_.append(reward_tensor)\n",
        "\n",
        "    training_reward += float(reward)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    runs.log({\n",
        "        \"global_step\": global_step\n",
        "    })\n",
        "\n",
        "    episodic_step += 1\n",
        "    global_step += 1\n",
        "\n",
        "    if global_step%cfg.oldPolicy_updationStep==0:\n",
        "      oldPolicy.load_state_dict(currentPolicy.state_dict())\n",
        "\n",
        "    if global_step%cfg.eval_steps==0:\n",
        "\n",
        "      if global_step%cfg.cam_counter==0:\n",
        "        rec = True\n",
        "      else:\n",
        "        rec = False\n",
        "\n",
        "      eval_reward, eval_steps = evaluationLoop(policynetwork=currentPolicy, recordVideo=rec)\n",
        "      runs.log(\n",
        "          {\n",
        "              \"avg-eval_rewards\": eval_reward,\n",
        "              \"eval-episodic-step\": eval_steps\n",
        "          }\n",
        "      )\n",
        "      currentPolicy.train()\n",
        "\n",
        "  all_states = torch.stack(all_states_)\n",
        "  all_log_probs = torch.stack(all_log_probs_).view(-1, 1)\n",
        "  all_rewards = torch.stack(all_rewards_).view(-1, 1)\n",
        "\n",
        "\n",
        "  Gt = 0\n",
        "\n",
        "  R = []\n",
        "\n",
        "  for r in reversed(all_rewards):\n",
        "    Gt = r + cfg.discount_factor * Gt\n",
        "    R.insert(0, Gt)\n",
        "\n",
        "  Rt = torch.stack(R).view(-1, 1)\n",
        "  Vt = ValueNetwork(all_states)\n",
        "\n",
        "  At = Rt - Vt.detach()\n",
        "\n",
        "  At = (At - At.mean()) / (At.std() +1e-5)\n",
        "\n",
        "  oldPolicyProb = oldPolicy.log_probs(all_states)[1]\n",
        "\n",
        "  r = torch.exp(all_log_probs - oldPolicyProb)\n",
        "\n",
        "  policy_loss = - torch.mean(torch.min(r * At, torch.clamp(r, 1 - cfg.epsilon, 1 + cfg.epsilon) * At))\n",
        "\n",
        "  policy_optimizer.zero_grad()\n",
        "  policy_loss.backward()\n",
        "  policy_optimizer.step()\n",
        "\n",
        "  value_loss = torch.nn.functional.mse_loss(Vt, Rt.detach())\n",
        "\n",
        "  value_optimizer.zero_grad()\n",
        "  value_loss.backward()\n",
        "  value_optimizer.step()\n",
        "\n",
        "  runs.log(\n",
        "      {\n",
        "      \"episode-step\" : episodic_step,\n",
        "      \"training-rewards\": training_reward,\n",
        "      \"actor-loss\": policy_loss.item(),\n",
        "      \"value-loss\": value_loss.item()\n",
        "      }\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "7CdqyMjtSi06",
        "outputId": "a53b5640-92ba-4275-a2a1-af039d146bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260131_210759-rlfylqro</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ajheshbasnet-kpriet/ppo/runs/rlfylqro' target=\"_blank\">ppo</a></strong> to <a href='https://wandb.ai/ajheshbasnet-kpriet/ppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ajheshbasnet-kpriet/ppo' target=\"_blank\">https://wandb.ai/ajheshbasnet-kpriet/ppo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ajheshbasnet-kpriet/ppo/runs/rlfylqro' target=\"_blank\">https://wandb.ai/ajheshbasnet-kpriet/ppo/runs/rlfylqro</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 53/100000 [03:10<89:18:11,  3.22s/it]/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "  0%|          | 108/100000 [06:24<75:40:38,  2.73s/it]/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "  0%|          | 483/100000 [28:17<97:09:41,  3.51s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1214719294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       \u001b[0meval_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluationLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicynetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrentPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecordVideo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m       runs.log(\n\u001b[1;32m     59\u001b[0m           {\n",
            "\u001b[0;32m/tmp/ipython-input-803361163.py\u001b[0m in \u001b[0;36mevaluationLoop\u001b[0;34m(policynetwork, recordVideo)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicynetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0meval_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1219313429.py\u001b[0m in \u001b[0;36mlog_probs\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0maction_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1219313429.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluationLoop(currentPolicy, True)"
      ],
      "metadata": {
        "id": "PM83lkbpXjmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7VsuVg8pZcq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}