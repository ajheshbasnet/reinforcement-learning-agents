{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NSQT9cmkLW_a"
      },
      "outputs": [],
      "source": [
        "! pip install swig \"gymnasium[box2d]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPpgmu6bbnCV"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "datKnudtXc1D"
      },
      "source": [
        "# **IMPORTING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "592W2ND6Vrki"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import deque\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "# from gymnasium.vector import SyncVectorEnv\n",
        "from gymnasium.wrappers import RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_JNsAjzZ5T-"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"MUJOCO_GL\"] = \"egl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60iIOcw1J2vD"
      },
      "outputs": [],
      "source": [
        "# def create_environment(cfgs, eval = False):\n",
        "\n",
        "#   def _init():\n",
        "#       env = gym.make( id=cfgs.id , render_mode=\"rgb_array\", max_episode_steps=cfg.max_steps)\n",
        "#       return env\n",
        "\n",
        "#   return _init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7ghF0Mvsxjh"
      },
      "outputs": [],
      "source": [
        "def create_environment(cfgs, eval = False):\n",
        "  env = gym.make( id=cfgs.id , render_mode=\"rgb_array\")\n",
        "  return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWGd2YkB2l8W"
      },
      "source": [
        "# **WANDB RUN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS0X6VK-W6C3"
      },
      "outputs": [],
      "source": [
        "def wandb_runs(cfg):\n",
        "\n",
        "  wandb.login(key = \"\")\n",
        "  run = wandb.init(\n",
        "    entity=\"ajheshbasnet-kpriet\",\n",
        "    project=\"ddpg\",\n",
        "    name = \"DDPG\",\n",
        "    config=vars(cfg),\n",
        "  )\n",
        "\n",
        "  return run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58FAgHd6XgNv"
      },
      "source": [
        "# **CONFIGURATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSCqFzICXY4j"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class configuration:\n",
        "  id = \"LunarLander-v3\"\n",
        "  n_rollouts = 25_000\n",
        "  eval_steps = 10\n",
        "  global_steps = 0\n",
        "  eval_loops = 3\n",
        "  batch_size = 96\n",
        "  ppo_r_clamp = 0.2\n",
        "  critic_lr = 1e-4\n",
        "  actor_lr = 1e-4\n",
        "  record_video = 500_000\n",
        "  global_eval_steps = 0\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = configuration()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be7coiwP20R-"
      },
      "source": [
        "**SyncVectorEnv so that we can run the n-environments parrallelly and utilize the GPUs because single environment is wayy poor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpR1buugKk6H"
      },
      "outputs": [],
      "source": [
        "# envs = SyncVectorEnv([create_environment(cfg) for _ in range(cfg.n_envs)])\n",
        "\n",
        "envs = create_environment(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GEyTf3l4UWy"
      },
      "outputs": [],
      "source": [
        "envs.reset()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REzQ_aBy2u5d"
      },
      "source": [
        "**Checking environment is working or not:)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy_OPT6e3O1q"
      },
      "source": [
        "# **Actor and Critic Netowrk**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh8HBQTyOTA1"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, action_dim):\n",
        "    super().__init__()\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Linear(input_dim, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, action_dim),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sequential(x)\n",
        "    return x\n",
        "\n",
        "  def get_log_probs_action(self, x, actions = None):\n",
        "    logits = self(x)\n",
        "\n",
        "    dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "    if actions == None:\n",
        "      action = dist.sample()\n",
        "\n",
        "    else:\n",
        "      action = actions\n",
        "\n",
        "    log_prob = dist.log_prob(action)\n",
        "    entropy = dist.entropy().mean()\n",
        "\n",
        "    return action, log_prob, entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPaVL6a9O950"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Linear(input_dim, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sequential(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbTU-sNyPUeO"
      },
      "outputs": [],
      "source": [
        "actornet = Actor(envs.observation_space.shape[0], envs.action_space.n).to(cfg.device)  #type: ignore\n",
        "criticnet = Critic(envs.observation_space.shape[0]).to(cfg.device)  #type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mji3FfE4P1HP"
      },
      "outputs": [],
      "source": [
        "print(f'''Parameters:\n",
        "==============================\n",
        "actor-network     : {sum(p.numel() for p in actornet.parameters())/1e3} k\n",
        "critic-network(s) : {sum(p.numel() for p in criticnet.parameters())/ 1e3} k\n",
        "==============================\n",
        "      ''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MWMoXRF3Tqu"
      },
      "source": [
        "**Evaluation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7xFx7s0WpLC"
      },
      "outputs": [],
      "source": [
        "def evaluation(actornet, record_video = False):\n",
        "\n",
        "  eval_env = gym.make(id = cfg.id, render_mode = 'rgb_array')\n",
        "  if record_video:\n",
        "    video_dir = f\"videos/{int(time.time())}\"\n",
        "    eval_env = RecordVideo(eval_env,  video_folder=video_dir, episode_trigger=lambda ep: True)\n",
        "\n",
        "  net_reward = 0\n",
        "  net_step = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for _ in range(cfg.eval_loops):\n",
        "\n",
        "      done = False\n",
        "\n",
        "      episodic_reward = 0\n",
        "      episodic_step = 0\n",
        "      state = eval_env.reset()[0]\n",
        "\n",
        "      while not done:\n",
        "\n",
        "        stateT = torch.tensor(state, dtype=torch.float32, device=cfg.device)\n",
        "        action = actornet(stateT).argmax().item()\n",
        "        nxt_state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "        done = terminated or truncated\n",
        "        state = nxt_state\n",
        "\n",
        "        episodic_reward += float(reward)\n",
        "        episodic_step += 1\n",
        "      print(episodic_reward)\n",
        "      net_reward += episodic_reward\n",
        "      net_step  += episodic_step\n",
        "\n",
        "\n",
        "  net_reward = net_reward / cfg.eval_loops\n",
        "  net_step = net_step / cfg.eval_loops\n",
        "\n",
        "  eval_env.close()\n",
        "\n",
        "  return net_reward, net_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2hkd4Aw6SL5"
      },
      "outputs": [],
      "source": [
        "evaluation(actornet, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs-HECi93Wby"
      },
      "source": [
        "**To sample the batches**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJwbt4RpR9kH"
      },
      "outputs": [],
      "source": [
        "actor_optimizer = torch.optim.AdamW(actornet.parameters(), lr = cfg.actor_lr)\n",
        "critic_optimizer = torch.optim.AdamW(criticnet.parameters(), lr = cfg.critic_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-aIg5JqZ9zR"
      },
      "source": [
        "**W&B RUNS TO LOG THE METRICS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY4z2rMkXM4v"
      },
      "outputs": [],
      "source": [
        "runs = wandb_runs(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmTbXFe-BD3U"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99\n",
        "lambda_ = 0.96\n",
        "entropy_beta = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU_YjGWi3biG"
      },
      "source": [
        "# **Heart & Core of the notebook: PPO Algorithm's Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnYrCFL0XspN"
      },
      "outputs": [],
      "source": [
        "for _ in tqdm(range(cfg.n_rollouts)):\n",
        "\n",
        "  done = False\n",
        "\n",
        "  states = []\n",
        "  actions = []\n",
        "  next_states = []\n",
        "  rewards = []\n",
        "  log_probs = []\n",
        "  dones = []\n",
        "\n",
        "  state = envs.reset()[0]\n",
        "\n",
        "  stateT = torch.tensor(state, dtype=torch.float32, device = cfg.device)\n",
        "  training_reward = 0\n",
        "  training_step = 0\n",
        "\n",
        "  while not done:\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      action, logprob, _ = actornet.get_log_probs_action(stateT, None)\n",
        "\n",
        "    next_state, reward, terminated, truncated, _ = envs.step(action=action.item())\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    next_stateT = torch.tensor(next_state, dtype=torch.float32, device = cfg.device)\n",
        "    rewardT = torch.tensor(reward, dtype=torch.float32, device = cfg.device)\n",
        "    doneT = torch.tensor(done, dtype=torch.float32, device = cfg.device)\n",
        "\n",
        "    states.append(stateT)\n",
        "    actions.append(action)\n",
        "    next_states.append(next_stateT)\n",
        "    log_probs.append(logprob)\n",
        "    rewards.append(rewardT)\n",
        "    dones.append(doneT)\n",
        "\n",
        "    stateT = next_stateT\n",
        "    training_reward += float(reward)\n",
        "    training_step += 1\n",
        "    cfg.global_steps += 1\n",
        "\n",
        "\n",
        "  runs.log({\"training-reward\": training_reward, \"training-step\": training_step, \"global-steps\": cfg.global_steps})\n",
        "\n",
        "  all_states = torch.stack(states)\n",
        "  all_actions = torch.stack(actions).view(-1, 1)\n",
        "  all_next_states = torch.stack(next_states)\n",
        "  all_rewards = torch.stack(rewards).view(-1, 1)\n",
        "  all_dones = torch.stack(dones).view(-1, 1)\n",
        "  old_log_probs = torch.stack(log_probs).view(-1, 1)\n",
        "\n",
        "    # ----- Compute Values -----\n",
        "  with torch.no_grad():\n",
        "      values = criticnet(all_states).view(-1, 1)           # [T, 1]\n",
        "      next_value = criticnet(all_next_states[-1]).view(1, 1)  # [1, 1]\n",
        "\n",
        "  # ----- GAE -----\n",
        "  T = all_rewards.size(0)\n",
        "\n",
        "  advantages = torch.zeros_like(all_rewards)\n",
        "  gae = 0\n",
        "\n",
        "  for t in reversed(range(T)):\n",
        "      if t == T - 1:\n",
        "          next_val = next_value\n",
        "      else:\n",
        "          next_val = values[t + 1]\n",
        "\n",
        "      delta = all_rewards[t] + gamma * (1 - all_dones[t]) * next_val - values[t]\n",
        "\n",
        "      gae = delta + gamma * lambda_ * (1 - all_dones[t]) * gae\n",
        "      advantages[t] = gae\n",
        "\n",
        "  returns = advantages + values\n",
        "  advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "\n",
        "  log_actor_loss = 0\n",
        "  log_policy_loss = 0\n",
        "  log_critic_loss = 0\n",
        "  log_entropy = 0\n",
        "  log_advantage = 0\n",
        "  step = 0\n",
        "\n",
        "  batch_rollout = int(all_next_states.size(0) / cfg.batch_size)\n",
        "\n",
        "  shuffled_ids = torch.randperm(all_states.size(0), device=cfg.device)\n",
        "\n",
        "  batch_size = min(cfg.batch_size, all_states.size(0))\n",
        "\n",
        "  for i in range(0, all_actions.size(0), batch_size):\n",
        "\n",
        "    batch_ids = shuffled_ids[i: i+batch_size]\n",
        "\n",
        "    mb_states = all_states[batch_ids]\n",
        "    mb_rewards = all_rewards[batch_ids].squeeze(-1)\n",
        "    mb_actions = all_actions[batch_ids].squeeze(-1)\n",
        "    mb_advantages = advantages[batch_ids].squeeze(-1)\n",
        "    mb_returns = returns[batch_ids].squeeze(-1)\n",
        "    mb_old_log_probs = old_log_probs[batch_ids].squeeze(-1)\n",
        "\n",
        "    _, mb_new_log_probs, entropy = actornet.get_log_probs_action(mb_states, mb_actions)\n",
        "\n",
        "    ratio = torch.exp(mb_new_log_probs - mb_old_log_probs)\n",
        "\n",
        "    policy_loss_ = -torch.min(ratio * mb_advantages, torch.clamp(ratio, 1-cfg.ppo_r_clamp, 1+cfg.ppo_r_clamp)*mb_advantages).mean()\n",
        "\n",
        "    policy_loss = policy_loss_ - entropy_beta * entropy\n",
        "\n",
        "    critic_loss = torch.nn.functional.mse_loss(criticnet(mb_states).squeeze(-1), mb_returns)\n",
        "\n",
        "    log_actor_loss += policy_loss.item()\n",
        "    log_critic_loss += critic_loss.item()\n",
        "    log_entropy += entropy.item()\n",
        "    log_policy_loss += policy_loss_.item()\n",
        "    log_advantage += mb_advantages.mean().item()\n",
        "    step += 1\n",
        "\n",
        "    actor_optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    actor_optimizer.step()\n",
        "\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    critic_optimizer.step()\n",
        "\n",
        "  log_actor_loss = log_actor_loss / step\n",
        "  log_policy_loss = log_policy_loss / step\n",
        "  log_critic_loss = log_critic_loss / step\n",
        "  log_entropy = log_entropy / step\n",
        "\n",
        "  runs.log({\"policy-loss\": log_policy_loss, \"actor-loss\": log_actor_loss, \"critic-loss\": log_critic_loss, \"entropy\": log_entropy, \"advantage\": log_advantage})\n",
        "\n",
        "  if cfg.global_eval_steps%cfg.eval_steps==0 and cfg.global_eval_steps>1:\n",
        "    rec_frame = True if cfg.global_steps% cfg.record_video == 0 else False\n",
        "    net_reward, net_step = evaluation(actornet, rec_frame)\n",
        "    runs.log({\"eval-reward\": net_reward, \"eval-steps\": net_step})\n",
        "  cfg.global_eval_steps += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JN0oR0sEg86"
      },
      "source": [
        "# **SAVE MODEL-WEIGHTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar2B43DxwHlE"
      },
      "outputs": [],
      "source": [
        "torch.save(actornet.state_dict(), \"policy-weights.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyFfWokiEE_g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
