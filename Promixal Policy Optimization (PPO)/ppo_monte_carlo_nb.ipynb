{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rbQDcy-tLQtn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ale_py\n",
        "import wandb\n",
        "import torch.nn as nn\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation, RecordVideo\n",
        "from gymnasium.vector import SyncVectorEnv\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "U3fPZ5BWmnqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c4aa99-0af1-436b-80a5-9e1f7e5fbe06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33majheshbasnet\u001b[0m (\u001b[33majheshbasnet-kpriet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "wandb.login(\n",
        "    key = \"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W_2gx0H1La1_"
      },
      "outputs": [],
      "source": [
        "class configs:\n",
        "\n",
        "  game_id = \"RiverraidNoFrameskip-v4\"\n",
        "  max_step = 4000\n",
        "  stack_size = 4\n",
        "  n_episodes = 50_000\n",
        "  policy_lr = 3e-3\n",
        "  value_lr = 2.5e-3\n",
        "  discount_factor = 0.99\n",
        "  epsilon = 0.15\n",
        "  rollouts = 5_000\n",
        "  eval_steps = 20000\n",
        "  cam_counter = 100_000\n",
        "  num_envs = 6\n",
        "  eval_loops = 3\n",
        "  ppo_epochs = 5\n",
        "  minibatch_size = 16\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = configs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Pa7dYp9QmSFi"
      },
      "outputs": [],
      "source": [
        "def create_run(configs):\n",
        "    return wandb.init(\n",
        "    name = \"ppo\",\n",
        "    project=\"ppo\",\n",
        "    # Track hyperparameters and run metadata.\n",
        "    config=vars(configs)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VdSiiOSuO1IX"
      },
      "outputs": [],
      "source": [
        "def createEnvironment(cfg):\n",
        "\n",
        "  def _init():\n",
        "\n",
        "    env = gym.make(cfg.game_id, frameskip = 1, full_action_space=False, render_mode=\"rgb_array\", max_episode_steps=configs.max_step)\n",
        "\n",
        "    env = AtariPreprocessing(env, frame_skip=4, grayscale_obs=True, screen_size = 84)\n",
        "    # \"scale_obs\" means the pixels are scaled/normalised from 0 to 1 else it's in uint8 number--> keeping it False because to store it the float32 takes way huge memory so the training will be too much slow around 11s/iteration. Hence do it during the run time only.\n",
        "\n",
        "    env = FrameStackObservation(env, cfg.stack_size)\n",
        "    # it gives [frame(t-3), frame(t-2), frame(t-1), frame(t)] NOT [frame(t), frame(t+1), frame(t+2), frame(t+3)]\n",
        "\n",
        "    # during env.reset() it gives obs = stack of [obs, obs, obs, obs] which is the same frame during the first time\n",
        "    # so after the 1st action the stack becomes [f0, f0, f0, f1] and after another action it becomes [f0, f0, f1, f2] and so on.\n",
        "\n",
        "    return env\n",
        "\n",
        "  return _init\n",
        "\n",
        "\n",
        "def evalenv(cfg):\n",
        "\n",
        "    env = gym.make(cfg.game_id, frameskip = 1, full_action_space=False, render_mode=\"rgb_array\", max_episode_steps=configs.max_step)\n",
        "\n",
        "    env = AtariPreprocessing(env, grayscale_obs=True, screen_size = 84)\n",
        "    # \"scale_obs\" means the pixels are scaled/normalised from 0 to 1 else it's in uint8 number--> keeping it False because to store it the float32 takes way huge memory so the training will be too much slow around 11s/iteration. Hence do it during the run time only.\n",
        "\n",
        "    env = FrameStackObservation(env, cfg.stack_size)\n",
        "    # it gives [frame(t-3), frame(t-2), frame(t-1), frame(t)] NOT [frame(t), frame(t+1), frame(t+2), frame(t+3)]\n",
        "\n",
        "    # during env.reset() it gives obs = stack of [obs, obs, obs, obs] which is the same frame during the first time\n",
        "    # so after the 1st action the stack becomes [f0, f0, f0, f1] and after another action it becomes [f0, f0, f1, f2] and so on.\n",
        "\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0MddU5CbPArZ"
      },
      "outputs": [],
      "source": [
        "env = SyncVectorEnv([createEnvironment(cfg) for _ in range(cfg.num_envs)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OQVJHZiOMTQQ"
      },
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, action_space):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(cfg.stack_size, 32, kernel_size=5, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=4, stride=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.ffnn = nn.Sequential(\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, action_space)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x/255.)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.ffnn(x)\n",
        "    return x\n",
        "\n",
        "  def log_probs(self, x, pick_action):\n",
        "\n",
        "    action_probs = torch.nn.functional.softmax(self(x), dim = -1)\n",
        "\n",
        "    if pick_action ==  None:\n",
        "      action_idx = torch.multinomial(action_probs, 1)\n",
        "      log_prob = torch.gather(action_probs, -1, action_idx).log()\n",
        "      return action_idx, log_prob\n",
        "\n",
        "    if pick_action != None:\n",
        "      log_prob = torch.gather(action_probs, -1, pick_action).log()\n",
        "      return log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zl-j7x3OaCrZ"
      },
      "outputs": [],
      "source": [
        "class Value_Network(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(cfg.stack_size, 32, kernel_size=5, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=4, stride=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.ffnn = nn.Sequential(\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x/255.)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.ffnn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sGKFq3rKPsqV"
      },
      "outputs": [],
      "source": [
        "currentPolicy = PolicyNetwork(env.single_action_space.n).to(cfg.device)\n",
        "ValueNetwork = Value_Network().to(cfg.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MJPCgBNN3pCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2095285-3b2e-4643-852f-655298b63a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================================\n",
            "> Actor-Net:  177.84 k\n",
            "> Policy-Net: 176.74 k\n",
            "-----------------------------------------------------------------------\n",
            "> CUDA is being used\n",
            "=======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f'''\n",
        "=======================================================================\n",
        "> Actor-Net:  {sum(p.numel() for p in currentPolicy.parameters())/1e3 :.2f} k\n",
        "> Policy-Net: {sum(p.numel() for p in ValueNetwork.parameters())/1e3 :.2f} k\n",
        "-----------------------------------------------------------------------\n",
        "> {cfg.device.upper()} is being used\n",
        "=======================================================================\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p593bwSSjId0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tjaXt0gIRQgp"
      },
      "outputs": [],
      "source": [
        "def evaluationLoop(policynetwork, recordVideo = False):\n",
        "\n",
        "  eval_env = evalenv(cfg)\n",
        "\n",
        "  if recordVideo:\n",
        "    eval_env = RecordVideo(\n",
        "                           eval_env, video_folder=\"videos/\",\n",
        "                           episode_trigger=lambda episode_id: True, name_prefix=\"ppo\"\n",
        "                           )\n",
        "\n",
        "  total_eval_rewards = 0\n",
        "  total_eval_steps = 0\n",
        "\n",
        "  policynetwork.eval()\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for _ in range(configs.eval_loops):\n",
        "\n",
        "      obs, _ = eval_env.reset()\n",
        "      done = False\n",
        "\n",
        "      ep_reward = 0.0\n",
        "      ep_step = 0\n",
        "\n",
        "      while not done:\n",
        "\n",
        "        action = policynetwork(torch.tensor(obs, dtype=torch.long).unsqueeze(0).to(cfg.device)).argmax().item()\n",
        "        next_obs,reward, terminated, truncated, _ =  eval_env.step(action)\n",
        "        obs = next_obs\n",
        "        ep_reward += float(reward)\n",
        "        done = terminated or truncated\n",
        "        ep_step += 1\n",
        "\n",
        "      total_eval_rewards += ep_reward\n",
        "      total_eval_steps += ep_step\n",
        "\n",
        "    total_eval_rewards = total_eval_rewards / cfg.eval_loops\n",
        "    total_eval_steps = int(total_eval_steps / cfg.eval_loops)\n",
        "\n",
        "  eval_env.close()\n",
        "  policynetwork.train()\n",
        "  return total_eval_rewards, total_eval_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FOb3fNgQWImg"
      },
      "outputs": [],
      "source": [
        "policy_optimizer = torch.optim.Adam(currentPolicy.parameters(), cfg.policy_lr)\n",
        "value_optimizer = torch.optim.Adam(ValueNetwork.parameters(), cfg.value_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caqt4dZBfGYc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "408ccf9a-887c-42ac-c716-b40f2b0badb6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260204_084327-d0vgsy6v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ajheshbasnet-kpriet/ppo/runs/d0vgsy6v' target=\"_blank\">ppo</a></strong> to <a href='https://wandb.ai/ajheshbasnet-kpriet/ppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ajheshbasnet-kpriet/ppo' target=\"_blank\">https://wandb.ai/ajheshbasnet-kpriet/ppo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ajheshbasnet-kpriet/ppo/runs/d0vgsy6v' target=\"_blank\">https://wandb.ai/ajheshbasnet-kpriet/ppo/runs/d0vgsy6v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "runs = create_run(cfg)\n",
        "global_step = 0\n",
        "\n",
        "ValueNetwork.train()\n",
        "currentPolicy.train()\n",
        "\n",
        "for steps in range(cfg.n_episodes):\n",
        "\n",
        "    buffer = {\n",
        "        \"states\": [],\n",
        "        \"rewards\": [],\n",
        "        \"actions\": [],\n",
        "        \"log_probs\": [],\n",
        "        \"terminated\": [],\n",
        "        \"truncated\": []\n",
        "    }\n",
        "\n",
        "    states = env.reset()[0]\n",
        "    training_rewards = torch.zeros((cfg.num_envs,), device=cfg.device)\n",
        "\n",
        "    # ===================== ROLLOUT =====================\n",
        "    for rollouts in range(cfg.rollouts):\n",
        "\n",
        "        state_tensor = torch.from_numpy(states).to(cfg.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action, log_probs = currentPolicy.log_probs(state_tensor, None)\n",
        "            action = action.cpu().numpy().reshape(-1)\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        training_rewards += torch.tensor(reward, dtype=torch.float32, device=cfg.device)\n",
        "\n",
        "        buffer[\"states\"].append(state_tensor.cpu())\n",
        "        buffer[\"actions\"].append(torch.tensor(action, dtype=torch.long))\n",
        "        buffer[\"rewards\"].append(torch.tensor(reward, dtype=torch.float32))\n",
        "        buffer[\"log_probs\"].append(log_probs.view(-1).cpu())\n",
        "        buffer[\"terminated\"].append(torch.tensor(terminated, dtype=torch.bool))\n",
        "        buffer[\"truncated\"].append(torch.tensor(truncated, dtype=torch.bool))\n",
        "\n",
        "        states = next_state\n",
        "        global_step += 1\n",
        "\n",
        "        runs.log({\"global-step\": global_step})\n",
        "\n",
        "        if global_step % cfg.eval_steps == 0:\n",
        "            rec = global_step % cfg.cam_counter == 0\n",
        "            eval_rewards, eval_steps = evaluationLoop(currentPolicy, rec)\n",
        "            runs.log({\"eval-rewards\": eval_rewards, \"eval-steps\": eval_steps})\n",
        "\n",
        "    # ===================== FLATTEN =====================\n",
        "    all_states = torch.stack(buffer[\"states\"]).permute(1, 0, 2, 3, 4).reshape(-1, 4, 84, 84).to(cfg.device)\n",
        "    all_actions = torch.stack(buffer[\"actions\"]).permute(1, 0).reshape(-1).to(cfg.device)\n",
        "    all_rewards = torch.stack(buffer[\"rewards\"]).permute(1, 0).reshape(-1).to(cfg.device)\n",
        "    old_log_prob = torch.stack(buffer[\"log_probs\"]).permute(1, 0).reshape(-1, 1).to(cfg.device)\n",
        "    all_terminated = torch.stack(buffer[\"terminated\"]).permute(1, 0).reshape(-1)\n",
        "    all_truncated = torch.stack(buffer[\"truncated\"]).permute(1, 0).reshape(-1)\n",
        "\n",
        "    # ===================== RETURNS =====================\n",
        "    Vt = ValueNetwork(all_states).view(-1, 1)\n",
        "    R = []\n",
        "    Gt = torch.zeros(1, device=cfg.device)\n",
        "    T = all_states.size(0)\n",
        "    idx = 0\n",
        "\n",
        "    for r, term, trunc in zip(\n",
        "        all_rewards.flip(0),\n",
        "        all_terminated.flip(0),\n",
        "        all_truncated.flip(0)\n",
        "    ):\n",
        "        idx += 1\n",
        "        if trunc.item():\n",
        "            Gt = Vt[T - idx].detach()\n",
        "        if term.item():\n",
        "            Gt = torch.zeros(1, device=cfg.device)\n",
        "\n",
        "        Gt = r + cfg.discount_factor * Gt\n",
        "        R.insert(0, Gt)\n",
        "\n",
        "    Rt = torch.stack(R)\n",
        "\n",
        "    # ===================== ADVANTAGE =====================\n",
        "    At = (Rt - Vt).detach()\n",
        "    At = (At - At.mean()) / (At.std() + 1e-8)\n",
        "\n",
        "    # ===================== PPO UPDATE =====================\n",
        "    N = all_states.size(0)\n",
        "\n",
        "    for _ in range(cfg.ppo_epochs):\n",
        "\n",
        "        indices = torch.randperm(N)\n",
        "\n",
        "        for start in range(0, N, cfg.ppo_epochs):\n",
        "            end = start + cfg.minibatch_size\n",
        "            mb_idx = indices[start:end]\n",
        "\n",
        "            mb_states   = all_states[mb_idx]\n",
        "            mb_actions  = all_actions[mb_idx]\n",
        "            mb_old_logp = old_log_prob[mb_idx]\n",
        "            mb_adv      = At[mb_idx]\n",
        "            mb_returns  = Rt[mb_idx]\n",
        "\n",
        "            mb_new_logp = currentPolicy.log_probs(\n",
        "                mb_states, mb_actions.reshape(-1, 1)\n",
        "            ).reshape(-1, 1)\n",
        "\n",
        "            ratio = torch.exp(mb_new_logp - mb_old_logp)\n",
        "\n",
        "            policy_loss = -torch.mean(\n",
        "                torch.min(\n",
        "                    ratio * mb_adv,\n",
        "                    torch.clamp(ratio, 1 - cfg.epsilon, 1 + cfg.epsilon) * mb_adv\n",
        "                )\n",
        "            )\n",
        "\n",
        "            value_pred = ValueNetwork(mb_states).view(-1, 1)\n",
        "            value_loss = torch.nn.functional.mse_loss(value_pred, mb_returns)\n",
        "\n",
        "            policy_optimizer.zero_grad()\n",
        "            policy_loss.backward()\n",
        "            policy_optimizer.step()\n",
        "\n",
        "            value_optimizer.zero_grad()\n",
        "            value_loss.backward()\n",
        "            value_optimizer.step()\n",
        "\n",
        "    runs.log({\n",
        "        \"policy-loss\": policy_loss.item(),\n",
        "        \"value-loss\": value_loss.item(),\n",
        "        \"training-rewards\": training_rewards.mean().item()\n",
        "    })\n",
        "    del all_states, all_rewards, all_terminated, all_truncated, old_log_prob\n",
        "    del policy_loss, value_loss\n",
        "runs.finish()\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYH1l7wXfGVv"
      },
      "outputs": [],
      "source": [
        "mb_states.size(), mb_actions.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "offHy_OPBbBc"
      },
      "outputs": [],
      "source": [
        "currentPolicy.log_probs(all_states, all_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Goquoia9RpVh"
      },
      "outputs": [],
      "source": [
        "torch.randperm(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgbQK46mW9YX"
      },
      "outputs": [],
      "source": [
        "all_states.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_7Jt16sekHs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}