{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "H1Diam1dKxIM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random as random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from collections import deque\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import wandb as wandb\n",
        "import math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key = \"XXX\")\n",
        "\n",
        "def get_wandb_run(configs):\n",
        "  run = wandb.init(\n",
        "    name=\"CartPole-runs\",\n",
        "    config=vars(configs),\n",
        "  )\n",
        "  return run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-H7CSLIX4UC",
        "outputId": "75679e3e-7de7-4212-ff7b-2f0ea4cc0ea3"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "\n",
        "class Config:\n",
        "  input_values: int = 4\n",
        "  output_actions: int = 2\n",
        "  max_length: int = 250\n",
        "  max_episode: int = 2000\n",
        "  n_moves: int = 140\n",
        "  learningRates: float = 5e-3\n",
        "  updationStep: int = 750\n",
        "  batch_size: int = 64\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "2crz_MZuVbQD"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.config = config\n",
        "      self.seq = nn.Sequential(\n",
        "          nn.Linear(self.config.input_values, 128),\n",
        "          nn.LeakyReLU(0.00001),\n",
        "\n",
        "          nn.Linear(128,256),\n",
        "          nn.LeakyReLU(0.00001),\n",
        "\n",
        "          nn.Linear(256,128),\n",
        "          nn.LeakyReLU(0.00001),\n",
        "\n",
        "          nn.Linear(128,64),\n",
        "          nn.LeakyReLU(0.00001),\n",
        "\n",
        "          nn.Linear(64,self.config.output_actions)\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      return self.seq(x)"
      ],
      "metadata": {
        "id": "RjKQ1YBdVpY0"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = deque(maxlen=config.max_length)"
      ],
      "metadata": {
        "id": "iCSWeEQt5fFB"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QTrain = DQNetwork(config).to(config.device)\n",
        "QTarget = DQNetwork(config).to(config.device)\n",
        "\n",
        "QTarget.load_state_dict(QTrain.state_dict())"
      ],
      "metadata": {
        "id": "CsmcwSfoWfC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b253aa5-06df-4932-c23a-6023a08ac9bc"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chooseAction(x, QTrain, t):\n",
        "  max_eps = 1.0\n",
        "  min_eps = 0.0001\n",
        "  T_Max = config.n_moves * config.max_episode\n",
        "\n",
        "  eps = min_eps + 0.5*(max_eps - min_eps) * (1 + torch.cos(torch.tensor((t*math.pi) / T_Max )))\n",
        "\n",
        "  if random.random() < eps:   # exploration:\n",
        "    return torch.randint(0, 2, (1,)).item()\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      output_states = QTrain(x)\n",
        "      return torch.argmax(output_states, dim = -1).item()\n",
        "\n",
        "def create_env(name: str):\n",
        "  return gym.make(name)\n",
        "\n",
        "def saveData(current_state, action, rewards, new_state, done):\n",
        "  assert isinstance(state, np.ndarray), \"it is not ndarray\"\n",
        "  assert isinstance(new_state, np.ndarray), \"it is not ndarray\"\n",
        "  return replay_buffer.append(\n",
        "      (current_state, action, rewards, new_state, done)\n",
        "  )\n",
        "env = create_env(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "iTAV1fx2WDHb"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(len: int):\n",
        "  batches = random.sample(replay_buffer, len)\n",
        "  current_state, action, rewards, new_state, done = zip(*batches)\n",
        "  return current_state, action, rewards, new_state, done"
      ],
      "metadata": {
        "id": "HC_3x9ldls53"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = get_wandb_run(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "687bD_SC6yWv",
        "outputId": "11d659ec-4d5e-4012-8083-3d2ad06a4dbf"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251221_141311-00saq0zm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ajheshbasnet-kpriet/uncategorized/runs/00saq0zm' target=\"_blank\">CartPole-runs</a></strong> to <a href='https://wandb.ai/ajheshbasnet-kpriet/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ajheshbasnet-kpriet/uncategorized' target=\"_blank\">https://wandb.ai/ajheshbasnet-kpriet/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ajheshbasnet-kpriet/uncategorized/runs/00saq0zm' target=\"_blank\">https://wandb.ai/ajheshbasnet-kpriet/uncategorized/runs/00saq0zm</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QTrainoptimizers = torch.optim.AdamW(QTrain.parameters(), lr = config.learningRates)"
      ],
      "metadata": {
        "id": "hiTPDgBr8JaA"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "import os\n",
        "\n",
        "video_dir = \"videos\"\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "def evaluation(QTrain, training: bool =  False):\n",
        "\n",
        "  env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "  if training:\n",
        "    env = RecordVideo(\n",
        "        env,\n",
        "        video_folder=video_dir,\n",
        "        episode_trigger=lambda episode_id: True  # record every episode\n",
        "    )\n",
        "\n",
        "  obs, info = env.reset()\n",
        "  done = False\n",
        "  total_rewards = 0.0\n",
        "\n",
        "  while not done:\n",
        "    action = torch.argmax(QTrain(torch.tensor(obs, dtype=torch.float32).to(config.device)), dim = -1)\n",
        "    obs, reward, terminated, truncated, info = env.step(action.item())\n",
        "    done = terminated or truncated\n",
        "    total_rewards += reward\n",
        "\n",
        "  env.close()  # <-- REQUIRED\n",
        "  return total_rewards"
      ],
      "metadata": {
        "id": "FKubAi3Px89W"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_steps = 0\n",
        "wandb_logging_step: int = 500\n",
        "evaluation_step: int = 1000\n",
        "gamma = 0.98\n",
        "eval_itr = 5\n",
        "t = 0\n",
        "\n",
        "for episodes in tqdm(range(config.max_episode)):\n",
        "\n",
        "  state, info = env.reset()\n",
        "  total_rewards = 0.0\n",
        "\n",
        "  for moves in range(config.n_moves):\n",
        "\n",
        "    action = chooseAction(torch.tensor(state).to(config.device), QTrain, t)\n",
        "    t += 1\n",
        "    new_state, reward, terminated, truncation, _ = env.step(action)\n",
        "    done = terminated or truncation\n",
        "\n",
        "    total_rewards += reward       # type: ignore\n",
        "\n",
        "    saveData(state, action, reward, new_state, done)\n",
        "    state = new_state\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "    if len(replay_buffer) == config.max_length:\n",
        "      batches = get_batches(config.batch_size)\n",
        "      states, actions, rewards, new_states, dones = batches\n",
        "      states, actions, rewards, new_states, dones = torch.tensor(np.stack(states), dtype = torch.float32 ).to(config.device), torch.tensor(actions, dtype=torch.long).to(config.device),torch.tensor(rewards,dtype=torch.float32).to(config.device), torch.tensor(np.stack(new_states), dtype = torch.float32).to(config.device),torch.tensor(dones, dtype=torch.bool).to(config.device)\n",
        "\n",
        "      q_values = QTrain(states)  # [B, A]\n",
        "      q_sa = q_values.gather(-1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        q_vals = QTarget(new_states).max(1).values\n",
        "        target = rewards + gamma * q_vals * (1 - dones.int())\n",
        "\n",
        "      loss = torch.nn.functional.mse_loss(q_sa, target)\n",
        "\n",
        "      QTrainoptimizers.zero_grad()\n",
        "      loss.backward()\n",
        "      QTrainoptimizers.step()\n",
        "\n",
        "      global_steps +=1\n",
        "\n",
        "      if global_steps%config.updationStep==0:\n",
        "        QTarget.load_state_dict(QTrain.state_dict())\n",
        "\n",
        "      if global_steps%wandb_logging_step:\n",
        "        run.log({\"loss\": loss.item()})\n",
        "\n",
        "      if global_steps%evaluation_step==0:\n",
        "        total_evaluated = 0\n",
        "        for _ in range(eval_itr):\n",
        "          evaluated = evaluation(QTarget)\n",
        "          total_evaluated += evaluated\n",
        "        total_evaluated = total_evaluated/eval_itr\n",
        "        run.log({\"evaluation-reward\": total_evaluated})\n",
        "\n",
        "  run.log({\"total_rewards\": total_rewards})\n",
        "\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "iDwCATe-jto0",
        "outputId": "48ba0102-396b-4096-e58f-2ed5a2517d19"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [03:02<00:00, 10.93it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation-reward</td><td>▁▁▁▁▂▃▃▃▂▂▁▁▂▃▄▃▃▂▄▁▂▂▂▂▂▃▂▂▂█▂▃▄▅▂▂▂▂▇▂</td></tr><tr><td>loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁▁▂▁▁▁█▇</td></tr><tr><td>total_rewards</td><td>▂▃▂▁▅▄▁█▇▃▃▂▂▃▂▂▁▂▃▂▃▄▂▂▂▁▁▂▆▁▂▂▁▁▄▁▂▂▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation-reward</td><td>70.4</td></tr><tr><td>loss</td><td>0.76721</td></tr><tr><td>total_rewards</td><td>13</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CartPole-runs</strong> at: <a href='https://wandb.ai/ajheshbasnet-kpriet/uncategorized/runs/00saq0zm' target=\"_blank\">https://wandb.ai/ajheshbasnet-kpriet/uncategorized/runs/00saq0zm</a><br> View project at: <a href='https://wandb.ai/ajheshbasnet-kpriet/uncategorized' target=\"_blank\">https://wandb.ai/ajheshbasnet-kpriet/uncategorized</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251221_141311-00saq0zm/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(QTrain.state_dict(), \"weights.pt\")"
      ],
      "metadata": {
        "id": "rcsoKacZN-oq"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QTarget = QTarget.to(config.device)"
      ],
      "metadata": {
        "id": "GV2EhCgaN3lN"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import os\n",
        "\n",
        "video_dir = \"videos\"\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "def evaluation(QTarget):\n",
        "\n",
        "  env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "  env = RecordVideo(\n",
        "      env,\n",
        "      video_folder=video_dir,\n",
        "      episode_trigger=lambda episode_id: True  # record every episode\n",
        "  )\n",
        "\n",
        "  obs, info = env.reset()\n",
        "  done = False\n",
        "  total_rewards = 0.0\n",
        "\n",
        "  while not done:\n",
        "    action = torch.argmax(QTrain(torch.tensor(obs, dtype=torch.float32).to(config.device)), dim = -1)\n",
        "    obs, reward, terminated, truncated, info = env.step(action.item())\n",
        "    done = terminated or truncated\n",
        "    total_rewards += reward\n",
        "\n",
        "  env.close()  # <-- REQUIRED\n",
        "  return total_rewards"
      ],
      "metadata": {
        "id": "XjAukolDfxBp"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(QTrain)"
      ],
      "metadata": {
        "id": "yOL2JRArZnnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf8c473-85fc-4449-df1f-89dc1ea188fd"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81.0"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pZ2svck4sbDi"
      },
      "execution_count": 225,
      "outputs": []
    }
  ]
}